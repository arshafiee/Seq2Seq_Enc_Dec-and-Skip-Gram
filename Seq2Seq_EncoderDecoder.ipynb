{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d0m0D9epfchF"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "from keras.utils import to_categorical\n",
    "import tensorboardcolab\n",
    "import os\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import collections\n",
    "import pandas as pd\n",
    "import string\n",
    "from string import digits\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, CuDNNLSTM, Input, Embedding, TimeDistributed, Flatten, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# using gpu\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MaDJnqqWg3Eb"
   },
   "source": [
    "### Mounting Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JNHprbeDfpzC"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a9t1Nq3lhCR7"
   },
   "source": [
    "### Number of Different Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IIK99SAAhnsA"
   },
   "outputs": [],
   "source": [
    "with open('drive/My Drive/Colab Notebooks/Problem1_HW4/ferdosi.txt', 'r') as file:\n",
    "  whole_list = text_to_word_sequence(file.read())\n",
    "  whole_list = [''.join(whole_list)] \n",
    "  chars = list(set(whole_list[0]))\n",
    "  vocab_size = len(chars)\n",
    "  print('data has {0} unique characters.'.format(vocab_size + 1))\n",
    "  vocab_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "  vocab_to_int[' '] = 36\n",
    "  vocab_to_int['<PAD>'] = 37\n",
    "  vocab_to_int['<GO>'] = 38\n",
    "  vocab_to_int['<END>'] = 39\n",
    "\n",
    "  int_to_vocab = { i:ch for i,ch in enumerate(chars) }\n",
    "  int_to_vocab[36] = ' '\n",
    "  int_to_vocab[37] = '<PAD>'\n",
    "  int_to_vocab[38] = '<GO>'\n",
    "  int_to_vocab[39] = '<END>'\n",
    "\n",
    "del(whole_list)\n",
    "del(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Va_n1FGFxFNH"
   },
   "source": [
    "#### Report:\n",
    "The Whole text is readed and splitted to a list of words. Number of Unique characters is calcualted. 'vocab_to_int' and 'int_to_vocab' dictionaries are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gMJpJ7Y5iW5D"
   },
   "outputs": [],
   "source": [
    "vocab_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRSQ5Xn3Gfs5"
   },
   "outputs": [],
   "source": [
    "int_to_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e8gYkfscXp4P"
   },
   "source": [
    "### Preparing Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "me8c3dUDSy5R"
   },
   "outputs": [],
   "source": [
    "# TODO: space at the end of each input\n",
    "input_text = []  # text of every input\n",
    "target_text = []  # text of every target\n",
    "\n",
    "inputs = []  # integer seq of every input\n",
    "targets = []  # integer seq of every target\n",
    "\n",
    "with open('drive/My Drive/Colab Notebooks/Problem1_HW4/ferdosi.txt', 'r') as file:\n",
    "  text_line = file.readlines()\n",
    "  print(len(text_line))\n",
    "  for line in text_line:\n",
    "    current_text = line.split(',')[0].rstrip()\n",
    "    input_text.append(current_text)\n",
    "    inputs.append([vocab_to_int[ch] for ch in current_text])\n",
    "\n",
    "    current_text = line.split(',')[1].rstrip()[1:]\n",
    "    target_text.append(current_text)\n",
    "    targets.append([vocab_to_int[ch] for ch in current_text])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SEoH5gkly90K"
   },
   "source": [
    "#### Report:\n",
    "At first, lines of the text are readed. We save first part of every line in 'input_text' and second part in 'target_text'. We also save their numeric sequence in 'inputs' and 'targets'(Extra spaces at the end of first part and the begining of second part are excluded)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t0g916TRAH_1"
   },
   "source": [
    "### Finding Maximum Number of Characters in a Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u3RdQPUVTDLn"
   },
   "outputs": [],
   "source": [
    "max_len_inputs = max([len(item) for item in inputs])\n",
    "max_len_targets = max([len(item) for item in targets])\n",
    "print('Maximum Length of Input Sequences : {0}'.format(max_len_inputs))\n",
    "print('Maximum Length of Target Sequences : {0}'.format(max_len_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lH2mUqgMzn0x"
   },
   "source": [
    "#### Report:\n",
    "Now we find maximum length of both input and target sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dGxS9OLlCPzT"
   },
   "source": [
    "### Padding Inputs and Targets And Performing One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1djf9lOeCV4F"
   },
   "outputs": [],
   "source": [
    "# number 37 as the '<PAD>' number! \n",
    "# number 38 as the '<GO>' number in the sequence!\n",
    "# number 39 as the '<END>' number in the sequence!\n",
    "for item in range(len(inputs)):\n",
    "  # padding inputs  \n",
    "  inputs[item] = [vocab_to_int['<PAD>']] * (max_len_inputs - len(inputs[item])) + inputs[item] \n",
    "  inputs[item] = [vocab_to_int['<GO>']] + inputs[item] + [vocab_to_int['<END>']]\n",
    "  \n",
    "  # padding targets  \n",
    "  targets[item] = targets[item] + [vocab_to_int['<PAD>']] * (max_len_targets - len(targets[item]))\n",
    "  targets[item] = [vocab_to_int['<GO>']] + targets[item] + [vocab_to_int['<END>']]\n",
    "  \n",
    "# one-hot encoding\n",
    "# oh_inputs = to_categorical(inputs)\n",
    "# oh_targets = to_categorical(targets)\n",
    "\n",
    "len_inputs = max([len(item) for item in inputs])\n",
    "len_targets = max([len(item) for item in targets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e-WWkkFi0CKH"
   },
   "source": [
    "#### Report:\n",
    "At first, We use padding to equalize the length of all input sequences and also equalize the length of all target sequences. we also add GO and END characters to all input and target sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k4pOAE854UjR"
   },
   "source": [
    "### Inputs And Targets of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9s57MseOsJ6N"
   },
   "outputs": [],
   "source": [
    "encoder_inputs_data = np.array(inputs)\n",
    "decoder_inputs_data = np.array(targets)[:, :-1]\n",
    "decoder_targets_data = np.array(to_categorical(targets)[:, 1:, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8FSeNyOu0xJZ"
   },
   "source": [
    "#### Report:\n",
    "encoder_inputs_data is provided by converting inputs to numpy array. decoder_inputs_data and decoder_targerts_data are also provided excluding END and Go characters respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FxgsR6r5Gvk5"
   },
   "source": [
    "### Splitting Test and Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_07z4K7QGzkB"
   },
   "outputs": [],
   "source": [
    "frac = round(0.9 * len(inputs))\n",
    "\n",
    "tr_en_inputs = encoder_inputs_data[:frac]\n",
    "tr_de_inputs = decoder_inputs_data[:frac]\n",
    "tr_de_targets = decoder_targets_data[:frac]\n",
    "\n",
    "ts_en_inputs = encoder_inputs_data[frac:]\n",
    "ts_de_inputs = decoder_inputs_data[frac:]\n",
    "ts_de_targets = decoder_targets_data[frac:]\n",
    "\n",
    "print('Shape of Dataset: \\t\\t{0}'.format(encoder_inputs_data.shape))\n",
    "print('Shape of Training Set Inputs: \\t{0}'.format(tr_en_inputs.shape))\n",
    "print('Shape of Training Set Targets: \\t{0}'.format(tr_de_targets.shape))\n",
    "print('Shape of Test Set Inputs: \\t{0}'.format(ts_en_inputs.shape))\n",
    "print('Shape of Test Set Targets: \\t{0}'.format(ts_de_targets.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R035AmyP1aCY"
   },
   "source": [
    "#### Report:\n",
    "We split test and train datasets. You can see their shapes above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "idaSot2LJVo0"
   },
   "source": [
    "### Building The Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QGaqLndFlGuv"
   },
   "outputs": [],
   "source": [
    "vec_len       = 300   # Length of the vector that we will get from the embedding layer\n",
    "latent_dim    = 1024  # Hidden layers dimension \n",
    "dropout_rate  = 0.2  # Rate of the dropout layers\n",
    "batch_size    = 64    # Batch size\n",
    "epochs        = 6    # Number of epochs\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "# Input layer of the encoder :\n",
    "encoder_input = Input(shape=(None,))\n",
    "\n",
    "# Hidden layers of the encoder :\n",
    "encoder_embedding = Embedding(input_dim = len(vocab_to_int), output_dim = vec_len)(encoder_input)\n",
    "encoder_dropout   = (TimeDistributed(Dropout(rate = dropout_rate)))(encoder_embedding)\n",
    "encoder_LSTM      = CuDNNLSTM(latent_dim, return_sequences=True)(encoder_dropout)\n",
    "\n",
    "# Output layer of the encoder :\n",
    "encoder_LSTM2_layer = CuDNNLSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_LSTM2_layer(encoder_LSTM)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wbVX7cht2Ukt"
   },
   "source": [
    "#### Report:\n",
    "In this part, we build our encoder. Input layer is constructed and after that we use Embeddings of size 300. We use Dropout and then we feed the outputs to our LSTM layer of size 1024. At last, another LSTM layer is provided and we save encoder outputs and encoder states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CPDZ6PrJr439"
   },
   "source": [
    "### Building The Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_7mHONf8r_Ma"
   },
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "# Input layer of the decoder :\n",
    "decoder_input = Input(shape=(None,))\n",
    "\n",
    "# Hidden layers of the decoder :\n",
    "decoder_embedding_layer = Embedding(input_dim = len(vocab_to_int), output_dim = vec_len)\n",
    "decoder_embedding = decoder_embedding_layer(decoder_input)\n",
    "\n",
    "decoder_dropout_layer = (TimeDistributed(Dropout(rate = dropout_rate)))\n",
    "decoder_dropout = decoder_dropout_layer(decoder_embedding)\n",
    "\n",
    "decoder_LSTM_layer = CuDNNLSTM(latent_dim, return_sequences=True)\n",
    "decoder_LSTM = decoder_LSTM_layer(decoder_dropout, initial_state = encoder_states)\n",
    "\n",
    "decoder_LSTM_2_layer = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_LSTM_2,_,_ = decoder_LSTM_2_layer(decoder_LSTM)\n",
    "\n",
    "# Output layer of the decoder :\n",
    "decoder_dense = Dense(len(vocab_to_int), activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_LSTM_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4IVk9UIY3YlV"
   },
   "source": [
    "#### Report:\n",
    "Here we build our decoder. At first, an input layer is generated and after that we use the embedding layer of size 300. Just like encoder model, we have Dropouts and 2 consecutive LSTM layers of size 1024. At last, we use a dense layer of size len(vocab_to_int) to convert the outputs to a domain of our dictionary size to make decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yL16FossY2S8"
   },
   "source": [
    "### Bringing The Encoder And Decoder Together Into One Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F7X9HzdaY87w"
   },
   "outputs": [],
   "source": [
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_input, decoder_input], decoder_outputs)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Define a checkpoint callback :\n",
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aqz6JsD04Lq5"
   },
   "source": [
    "#### Report:\n",
    "Here you can see the summary of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Zc-7v7ycZRJ"
   },
   "source": [
    "### Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M-hz2rIvcYEL"
   },
   "outputs": [],
   "source": [
    "# Run training\n",
    "num_train_samples = len(tr_en_inputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "hsitory = model.fit([tr_en_inputs[:num_train_samples,:],\n",
    "               tr_de_inputs[:num_train_samples,:]],\n",
    "               tr_de_targets[:num_train_samples,:,:],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.08,\n",
    "          callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "82rue3FD4b53"
   },
   "source": [
    "#### Report:\n",
    "We train our model by 'rmsprop' as our optimizer and 'categorical_crossentropy' as our loss function. You can see that our loss is decreasing and the accuracy of both validation and training datasets are increasing gradually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UP5hcgwuETW7"
   },
   "source": [
    "### Plotting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MbaE_LPBETCW"
   },
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model_plot4a.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8igjuVRO5JiZ"
   },
   "source": [
    "#### Report:\n",
    "Here you can see the structure of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vxFygMwHxp3a"
   },
   "source": [
    "### Loss & Accuracy Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rB7I57K3xpKF"
   },
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(hsitory.history['acc'], color='red', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='green', markersize=10)\n",
    "plt.plot(hsitory.history['val_acc'],  color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='yellow', markersize=10)\n",
    "plt.grid()\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(hsitory.history['loss'],  color='red', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='green', markersize=10)\n",
    "plt.plot(hsitory.history['val_loss'], color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='yellow', markersize=10)\n",
    "plt.grid()\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Q7U12Fx5e57"
   },
   "source": [
    "#### Report:\n",
    "Loss function and accuracy of both validation and training samples are plotted. You can see that the loss of our model has decreased along the epochs and accuracies of both training and validation datasets have increased gradually and we have accuracies more that 70% for both datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NyB5eKDG4b4p"
   },
   "source": [
    "### Test Loss & Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LIjOjExcrGnX"
   },
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate([ts_en_inputs,\n",
    "                                           ts_de_inputs],\n",
    "                                           ts_de_targets)\n",
    "train_loss, train_accuracy = model.evaluate([tr_en_inputs,\n",
    "                                           tr_de_inputs],\n",
    "                                           tr_de_targets)\n",
    "table = pd.DataFrame({'Dataset': ['Training Dataset', 'Test Dataset'],\n",
    "                           'Accuracy': [train_accuracy, test_accuracy],\n",
    "                           'Loss': [train_loss, test_loss]})\n",
    "display(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8JRXXjXW6IkY"
   },
   "source": [
    "#### Report:\n",
    "Accuracies and Values of loss function for both test and train samples are provided in the table above. As you can see, the accuracy of test dataset is so close to validation and training accuracy and we don't have the problem of ovefitting and the results are satisfactory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ekhfk8KJEgvi"
   },
   "source": [
    "### Inference Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mCk79NUQ7Izd"
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_input, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# Data flows through decoder\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding_layer(decoder_inputs_single)\n",
    "decoder_inputs_single_x = decoder_dropout_layer(decoder_inputs_single_x)\n",
    "decoder_preoutputs = decoder_LSTM_layer(decoder_inputs_single_x,\n",
    "                                  initial_state = decoder_states_inputs)\n",
    "decoder_outputs, h, c = decoder_LSTM_2_layer(decoder_preoutputs)\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_states = [h, c]\n",
    "\n",
    "# defining new decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs_single] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OL0Y8TBM6tgc"
   },
   "source": [
    "#### Report:\n",
    "Here we build our inference model. There is a small difference between inference and training model. In inference model we only feed GO character to decoder inputs and other inputs are the outputs of previous time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OjP5yMwyNwwJ"
   },
   "source": [
    "### Plotting Modified Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xs25omqnNvie"
   },
   "outputs": [],
   "source": [
    "plot_model(decoder_model, to_file='model_plot_dec.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HncLIlZS7pHy"
   },
   "source": [
    "#### Report:\n",
    "We plot the structure of our modified decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yNzkW8c3OoO5"
   },
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YBoa3UauOe13"
   },
   "outputs": [],
   "source": [
    "def predict_sentence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = vocab_to_int['<GO>']\n",
    "    eos = vocab_to_int['<END>']\n",
    "    output_sentence = []\n",
    "\n",
    "    for _ in range(len_targets - 1):\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        idx = np.argmax(output_tokens[0, 0, :])\n",
    "\n",
    "        \n",
    "\n",
    "        word = int_to_vocab[idx]\n",
    "        output_sentence.append(word)\n",
    "\n",
    "        if eos == idx:\n",
    "            break\n",
    "\n",
    "        target_seq[0, 0] = idx\n",
    "        states_value = [h, c]\n",
    "\n",
    "    output_sentence.reverse()\n",
    "    return ''.join(output_sentence)\n",
    "for _ in range(10):\n",
    "  i = np.random.choice(len(tr_en_inputs))\n",
    "  input_seq = tr_en_inputs[i]\n",
    "  prediction = predict_sentence(input_seq)\n",
    "  print('Input: ', input_text[i])\n",
    "  print('Response: ', prediction)\n",
    "  print('Correct Response: ', target_text[i])\n",
    "  print('------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Seq2Seq-EncoderDecoder.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
